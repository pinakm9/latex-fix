 Since we used deep learning to find zeros of the Fokker-Planck operator in the prequel \cite{mandal2023learning}, the most natural question becomes does an analogous algorithm work for time-dependent FPEs? For an overview of learning solutions to PDEs in a \textit{physics-informed} fashion see section 5 of \cite{mandal2023learning} or \cite{raissi2019physics}, \cite{blechschmidt2021three}, \cite{sirignano2018dgm}. 
As noted in section 5 of \cite{mandal2023learning} the first step to learning the solution to a PDE is to convert it into an optimization problem. For notational convenience let us first define the function space $\mathcal F$ as follows,
\begin{align}
\mathcal F\stackrel{\rm def}{=}\{&f:[0, T]\times\mathbb R^d\to[0, +\infty):\int_{\mathbb R^d}f(t, \mathbf x)\,d\mathbf x=1\;\forall\; t\in[0, T]\}\label{eq:def-fn-space--dynamic-fp}
\end{align}

Proceeding parallely to section 5 of \cite{mandal2023learning} or \cite{sirignano2018dgm} we might be tempted to solve the following optimization problem,
\begin{align}
    \underset{f\in\mathcal F }{\rm arg\,inf}\;\mathcal J(f)\stackrel{\rm def}{=}\underset{f\in\mathcal F }{\rm arg\,inf}\left[\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N\left(\frac{\partial f}{\partial t}(t_i, \mathbf x_j)-\mathcal Lf(t_i, \mathbf x_j)\right)^2+ \frac{1}{N}\sum_{j=1}^N(f(0, \mathbf x_j)-p_0(\mathbf x_j))^2\right]\label{eq:dynamic-opt--dynamic-fp}
\end{align}
where $\{\mathbf x_j\}_{j=1}^N$ is a uniform sample from our compact  domain of interest $\Omega_I$ (see section 6.1 in \cite{mandal2023learning}) and $\{t_i\}_{i=1}^M$ is a sample from $[0,T]$.
But it turns out problem \eqref{eq:dynamic-opt--dynamic-fp} is not a \textit{well-behaved} problem. The following proposition explains why this is the case.
\begin{prop}
    Suppose \eqref{eq:FPE-0--dynamic-fp} has a unique, strong solution $p$ and 
    \begin{align}
        \mathcal Lf = 0\label{eq:SFPE--dynamic-fp}
    \end{align}
    also has a unique, strong, probability solution $p_\infty$. 
    Then there exists a sequence of functions $f_k\in\mathcal F$ independent of the samples $\{\mathbf x_j\}_{j=1}^N, \,\{t_i\}_{i=1}^M$ such that \begin{align}
        \lim_{k\to+\infty}\mathcal J(f_k) = 0
\end{align}
    but
    \begin{align}
        \lim_{k\to+\infty}f_k\neq p
    \end{align}\label{prop:failure--dynamic-fp}
\end{prop}
\begin{proof}
WLOG assume that $\{t_i\}_{i=1}^M$ is ordered and $0=t_1<t_2<\cdots<t_M$. Since we have included $t_1=0$ in our sample, while considering $\frac{\partial}{\partial t}$ at $t=t_1=0$ we'll only consider the right derivative.
For $k\in\mathbb N$ define the sequence,
\begin{align}
    f_{k}(t, \mathbf x) =\begin{cases}
    &p(t,\mathbf x),\;\;t\le\frac{1}{k}\\
    &\phi_k(t)\,p(t, \mathbf x) + (1-\phi_{k}(t))\,p_\infty(\mathbf x),\;\; t>\frac{1}{k}
    \end{cases}
\end{align}
where,
\begin{align}
    \phi_{k}(t) = e^{1-kt}
\end{align}
Since $p,p_\infty$ are probability densities, it is easy to see that  $f_k\in\mathcal F,\;\;\forall\, k\in\mathbb N$.
The second term or the initial condition term in $\mathcal J(f_k)$ vanishes since $f_k(0,\mathbf x)=p(0,\mathbf x)=p_0(\mathbf x)$. Moreover since $p$ is a zero of $\frac{\partial}{\partial t}-\mathcal{L}$, 
\begin{align}
    \sum_{j=1}^N\left(\frac{\partial f_k}{\partial t}(t_1, \mathbf x_j)-\mathcal Lf_k(t_1, \mathbf x_j)\right)^2 =0
\end{align}
where as noted before we have only considered the right time-derivative at  $t=t_1=0$. Now pick $K>0:\frac{1}{K}<t_2$. For $k>K$, using the fact that $p, p_\infty$ are both zeros of $\frac{\partial}{\partial t}-\mathcal{L}$ we see that,
\begin{align}
    \mathcal J(f_{k})=&\frac{1}{MN}\sum_{i=2}^M\sum_{j=1}^N\left(\frac{d \phi_k}{d t}(t_i)(p_\infty(\mathbf x_j)-p(t_i, \mathbf x_j))\right)^2
\end{align}
Due to continuity of $p,p_\infty$ and compactness of $\Omega_I$ there exists $B\ge0:$ 
    \begin{align}
        \sup\left\{p(t, \mathbf x)^2, p_\infty(\mathbf x)^2 :t\in[0, T],\;\mathbf x\in\Omega_I\right\}= B
    \end{align}
which implies $\;\forall\;k>K$,
\begin{align}
    \mathcal{J}(f_k)\le&\frac{2B}{MN}\sum_{i=2}^M\sum_{j=1}^N\left(\frac{d \phi_k}{d t}(t_i)\right)^2=\frac{2Be^2k^2}{MN}\sum_{i=2}^M\sum_{j=1}^Ne^{-2kt_i}
<{2Be^2k^2}e^{-2kt_2}
\end{align}
Since $t_2>0$ we have
\begin{align}
    \lim_{k\to+\infty}\mathcal J(f_k)=0
\end{align}
But clearly the pointwise limit of $f_k$ is given by
\begin{align}
    f(t, \mathbf x) =\begin{cases}
    &p_0(\mathbf x),\;\;t=0\\
    &p_\infty(\mathbf x),\;\; t>0
    \end{cases}\label{eq:limit-min-seq--dynamic-fp}
\end{align}
\end{proof}
The proof assumes very little about the operator $\mathcal L$ and hence can be extended to other partial differential equations of parabolic type. Only assumptions required to prove proposition~\ref{prop:failure--dynamic-fp} are the existence of strong, unique probability solutions which are true for all of our example problems, see appendix 9.1 in \cite{mandal2023learning} and appendix~\ref{ssec-unique--dynamic-fp} in this paper. Since we only have finite computational resources it is sensible to investigate loss functions with finite sample-sizes and note that the proof of proposition~\ref{prop:failure--dynamic-fp} works for any choice of samples $\{t_i\}_{i=1}^M,\,\{\mathbf x_j\}_{j=1}^N$ with arbitrary but finite sample-sizes. Note that the pathological sequence we constructed is independent of the samples $\{t_i\}_{i=1}^M,\,\{\mathbf x_j\}_{j=1}^N$ which is an important component of the failure of the physics-informed way. In the case of stationary Fokker-Planck equations we can also construct pathological functions that make the physics-informed loss (defined in section~6.3 of \cite{mandal2023learning}) vanish without converging to a solution by simply interpolating a solution at the sample points with non-solutions at unsampled points. But the physics-informed method still produces correct solutions in the stationary case since these pathological functions are sample-dependent and as the sample-size increases, they resemble a solution more and more closely. From the perspective of training a neural network to optimize problem~\ref{eq:dynamic-opt--dynamic-fp} the main difficulty is revealed by the limit of the minimizing sequence given in \eqref{eq:limit-min-seq--dynamic-fp}. The network can behave like the initial condition for a short amount of time and then mimic $p_\infty$ for all subsequent times losing all variation in time. Moreover, one can construct many such sequences with pathological behaviors, not to mention restricting neural networks to $\mathcal{F}$ and have them be expressive is another challenge since normalization is an extremely difficult condition to implement in high dimensions, see the discussion in sections 2 and 6.4 of \cite{mandal2023learning}. Apart from these difficulties, for other explorations of failure modes of physics-informed neural networks see \cite{krishnapriyan2021characterizing}, \cite{basir2022investigating}.