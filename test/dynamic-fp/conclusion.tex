Physics-informed neural networks are unsuitable for a large class of partial differential equations of parabolic type. But algorithm~\ref{algo:hybrid--dynamic-fp} in conjunction with algorithm~6.1 in \cite{mandal2023learning} gives us a viable method for computing solutions to Fokker-Planck equations in high dimensions. The ability to focus on the solution pointwise lets us avoid the curse of dimensionality in a practical sense. To compute the Feynman-Kac expectaion we can compute the Euler-Maruyama trajectories in a highly parallel way and the same trajectories can be used to calculate solutions for different initial conditions. We are able to compute solutions for gradient systems up to arbitrarily long times. But for non-gradient system we encounter domain contraction, a phenomenon where we are able to calculate the solution on a subset of where know the stationary solution. Due to blow-up of the h-SDE for non-gradient systems we can compute solutions only up to a finite time. In case we have perfect knowledge of $p_\infty$, under ideal conditions the error that is introduced by letting some trajectories of h-SDE escape the region where we know the stationary solution is proportional to the fraction of trajectories that escape. Even after allowing a significant portion of trajectories to escape, we can achieve decent approximations. Moreover, algorithm~\ref{algo:hybrid--dynamic-fp} can achieve solutions that are comparable to Monte-Carlo solutions using orders of magnitude fewer trajectories for each point. The connection between the Fokker-Planck equation and the stochastic filtering problem suggests that modifications/extensions of algorithm~\ref{algo:hybrid--dynamic-fp} can be useful for devising new filtering algorithms.