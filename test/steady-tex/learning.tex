In this section we describe the general process of \textit{learning} a solution to a partial differential equation. The strategy described here will be an integral part of the final algorithm. In what follows next, we see how to see solve a generic PDE independent of time on a bounded domain with a Dirichlet boundary condition in a \textit{physics-informed} manner.  The interested reader can see \cite{raissi2019physics}, \cite{blechschmidt2021three}, \cite{sirignano2018dgm} for more discussions. In the next few subsections we keep simplifying our PDE problem until it finally becomes solvable on a computer.

\subsection{From PDE to optimization problem} In the context of machine learning, \textit{learning} refers to solving an optimization problem. So to solve our PDE with deep learning we first transform it into an optimization problem. Suppose our 2nd order PDE looks like, 
\begin{equation}
\begin{aligned}
    &\mathcal Lf(\mathbf x) = 0,\quad \mathbf x\in\Omega \,, \\
    &f(\mathbf x) = g(\mathbf x),\quad \mathbf x\in\partial\Omega \,,
\end{aligned}\label{eq:generic-pde}
\end{equation}
and just like before we are interested in finding a solution in $W^{1,2}(\Omega)\cap C^2(\Omega)$. Instead of trying to solve \eqref{eq:generic-pde} a popular strategy is to try to solve the following problem (see for example \cite{sirignano2018dgm}),
\begin{align}
    \min_{f\in W^{1,2}(\Omega)\cap C^2(\Omega)}\left[\int_\Omega (\mathcal L f)^2 + \int_{\partial\Omega}(f-g)^2\right]\label{eq:generic-pde-opt}
\end{align}
The choice of function space ensures one-to-one correspondence between the solutions of the PDE and the optimization problem.

\subsection{From infinite-dimensional search space to finite-dimensional search space}\label{ssec-infinite-to-finite} To solve a problem on a machine with finite resources we need to finitize the infinite aspects of the problem. We then solve the finitized problem which preferably approximates the original problem well to get an approximate solution to the orginal problem. For \eqref{eq:generic-pde-opt} our search space $W^{1,2}(\Omega)\cap  C^2(\Omega)$ is infinite dimensional which we need to replace with a finite dimensional search space.
In order to finitize the dimension of the search space we appeal to universal approximation theorems that say neural networks of even the simplest architectures are dense in continuous functions, see for example theorem 3.2 in \cite{kidger2020universal} or proposition 3.7 in \cite{pinkus1999approximation}. Universal approximation theorems typically allow networks to have either arbitrary depth or arbitrary width in order to achieve density \cite{pinkus1999approximation}, \cite{de2021approximation}. But the sets of neural networks with arbitrary depth or width are still infinite dimensional and therefore are infeasible to work with. In practice, we fix an architecture $\mathcal A$ with a fixed number of layers and trainable parameters and work with the following set instead.
\begin{align}
    S_{\mathcal A}\stackrel{\rm def}{=}\{n^{\mathcal A}_\theta: \theta\in \mathbb R^C\}\label{eq:search-space-net}
\end{align}
Here $n^{\mathcal A}_\theta$ is a network with architecture $\mathcal A$ with trainable parameters $\theta$ and $C$ is the total number of trainable parameters or the size of $\theta$. Since $C$ is fixed, $S_{\mathcal A}$ has a one-to-one correspondence with $\mathbb R^C$ and therefore is finite-dimensional. Even though we lose the density argument while working with $\theta$ of fixed size, in recent times it has been shown that sets like $S_\mathcal A$ are not closed in $W^{1, 2}(\Omega)$ and can, in principle, be used as good function approximators, see \cite{mahan2021nonclosedness} for a detailed discussion. In the following discussion we suppress the architecture and use $n^{\mathcal A}_\theta$ and $n_\theta$ interchangably for notational convenience. After restricting our search space to \eqref{eq:search-space-net},  \eqref{eq:generic-pde-opt} becomes,
\begin{align}
    \min_{\theta\in\mathbb R^C}\left[\int_{\Omega} (\mathcal L n_\theta)^2 + \int_{\partial\Omega}(n_\theta-g)^2\right]\label{eq:generic-pde-opt-theta}
\end{align}

\subsection{From integrals to sums} 
The domain $\Omega$ in our examples will often be of such a dimension that will make computing the integrals in \eqref{eq:generic-pde-opt-theta} extremely challenging. To deal with this we will replace the integrals in \eqref{eq:generic-pde-opt-theta} with Monte-Carlo sums as follows,

\begin{align}
    \min_{\theta\in\mathbb R^C}\left[\frac{1}{N}\sum_{j=1}^N (\mathcal L n_\theta(\mathbf x_j))^2 + \frac{1}{M}\sum_{j=1}^M(n_\theta(\mathbf y_j)-g(\mathbf y_j))^2\right]\label{eq:generic-opt-final}
\end{align}
where $\{\mathbf x_j\}_{j=1}^N$, $\{\mathbf y_j\}_{j=1}^M$ are uniform samples from $\Omega$ and $\partial \Omega$ respectively.

\subsection{Finding the optimal parameters}\label{ssec-finding-theta}
We simply perform gradient descent with respect to $\theta$ to find the optimal network for the problem \eqref{eq:generic-opt-final}. The Monte-Carlo sample sizes should be dictated by the hardware available to the practitioner.  In our experiments $N=M=1000$. In higher dimensions these choices are not enough to capture the original integrals entirely in one go. In that case \eqref{eq:generic-opt-final} can interpreted as trying to find a network that satisfies the original problem \eqref{eq:generic-pde} at the specified points $\{\mathbf x_j\}_{j=1}^N, \{\mathbf y_j\}_{j=1}^M$, which we can refer to as \textit{collocation points}. But in our experiments we try to the learn the solution on the entire domain as thoroughly as possible and so we resample the domain every few training iterations. So even though we are limited in sample-size by our hardware, we can shift the burden on space or memory to time or number of training iterations and adequately sample the entire domain. This principle of space-time trade-off is ubiquitous in machine learning \cite{buduma2022fundamentals} and comes in many different flavours like mini-batch gradient descent, stochastic gradient descent etc. Even though in this paradigm we are not training our network with typical input-output pairs, our method can be thought of as a variant of the mini-batch gradient descent.

\subsection{Why deep learning}
Deep learning in this context refers to learning an approximate solution to \eqref{eq:generic-pde} with the outlined method with an architecture $\mathcal A$ that is \textit{deep} or has many hidden layers. Deep networks are more efficient as approximators than shallow networks in the sense that they require far fewer number of trainable parameters to achieve the same level of approximation, for a discussion see section 5 of \cite{holstermann2023expressive}. Now that we have described the general procedure of \textit{deep learning} a solution to a PDE, we will pause briefly to point out some benefits and demerits of this approach. Deep learning has, like any other method some disadvantages. 
\begin{itemize}
    \item Deep learning is slower and less accurate for lower dimensional problems for which standard solvers exist and have been in consistent development for many decades. 
    \item Most modern GPUs are optimized for computation with single-precision or float32 numbers. This is efficient for rendering polygons or other image processing tasks which are the primary reasons GPUs were invented \cite{peddie2023history} but float32 might be not accurate enough for scientific computing. Although not ideal, this problem will most likely disappear in the future.
    \item The objective or \textit{loss} function used in a typical problem might not be convex and hence difficult to deal with \cite{krishnapriyan2021characterizing}, \cite{basir2022investigating}. 
\end{itemize}
But even with these disadvantages, the benefits of deep learning make it a worthwhile tool for solving PDEs.
\begin{itemize}
    \item Since we don't need to deal with meshes or grids in this method, we can mitigate the curse of dimensionality in memory. It will be clear from our experiments that the size of the network or $C$ does not need to grow exponentially with the dimensions. This method lets one compute the solution at collocation points but if one wants to compute the solution over the entire domain, one needs to sample the entire domain thoroughly which can be done in a sequential manner without requiring more memory as discussed in~\ref{ssec-finding-theta}.
    \item All derivatives are computed with automatic differentiation and therefore are accurate upto floating point errors. Moreover, finite difference schemes do not satisfy some fundamental properties of differentiation e.g. the product rule \cite{ranocha2019mimetic}. With automatic differentiation one does not have to deal with such problems. 
    \item If one computes the solution over the entire domain, the solution is obtained in a functional form which can be differentiated, integrated etc.
    \item Other than a method for sampling no modifications are required for accommodating different domains. 
\end{itemize}
