\subsection{Particle Filter} \label{ssec-pf--probing-nfs}
\begin{algorithm}[!t]
Initialize $N$ particles $\{x_0^i\}_{i=1}^N$ according to the initial distribution with equal weights $\left\{w_0^i=\frac{1}{N}\right\}_{i=1}^N$. Set $\tilde\sigma$. Below $S[i]$ denotes $i$-th element of $S$.\\
 \For{$k=0,\cdots,n$}{
      \If{$k>0$}{
      \For{$i=1,\cdots,N$}{
            $x^i_k\leftarrow f(S[i])$
            }
      }
      Sample $u\sim\mathcal{U}\left(0, \frac{1}{N}\right)$\\
      \For{$i=1,\cdots,N$}{
            $w^i_k\leftarrow p(y_k|x_k^i)$\\
            $U_{i} \leftarrow u + \frac{i-1}{N}$
            }
      $W\leftarrow\sum_{i=1}^Nw^i_k$\\
      \For{$i=1,\cdots,N$}{ 
            \If{$|\{U_j:\sum_{l=1}^{i-1}w^l_k \le WU_j\le\sum_{l=1}^{i}w^l_k\}|>0$}{
                    tag $x^i_k$ as significant
            }
            }
      Set $S\leftarrow\{x^{i_1}_k, x^{i_2}_k, \cdots, x^{i_m}_k\}$ as the set of  significant particles and compute $N_j\propto w^{i_j}_k:  \sum_{j=1}^mN_j = N$.\\
      \For{$j=1, \cdots, m$}{
            $S\leftarrow S\cup\{N_j-1\text{ samples from }\mathcal{N}(x^{i_j}_k, \tilde\sigma^2I_d)\}$
            }
    $\hat\pi_k \leftarrow\frac{1}{N}\sum_{i=1}^N\delta_{S[i]}$
  }
 \caption{BPF with offspring-based resampling}
\label{algo-bpf--probing-nfs}
\end{algorithm}
We use the bootstrap particle filter (BPF), algorithm~\ref{algo-bpf--probing-nfs}, resampling at every assimilation step. In order to avoid weight degeneracy in BPF for deterministic models, we incorporate post-regularization \cite{farchi2018comparison} in resampling steps by using an offspring based resampling strategy. We find the significant particles in the same manner as systematic resampling \cite{doucet2009tutorial} and then generate offspring for each of the significant particles by placing a Gaussian distribution with covariance $\tilde\sigma^2I_d$ around them. Number of offspring is set to be proportional to the weight of the particle. In algorithm~\ref{algo-bpf--probing-nfs}, we use the convention that $\sum_{l=1}^0w^l_k=0$.
\subsection{Ensemble Kalman filter (EnKF)} \label{ssec-enkf--probing-nfs}
\begin{algorithm}[!t]
Initialize $N$ particles $\{x_0^i\}_{i=1}^N$ according to the initial distribution and set $x_0^{i,a}=x_0^i$ \\
\For{$k=1,\cdots,n$}{
    \For{$i=1,\cdots,N$}{
        $x^{i,f}_{k}\leftarrow f(x_{k-1}^{i,a})$
    }
    $m_{k}^{f}\leftarrow \frac{1}{N}\sum_{i}x_{k}^{i,f}$\\ %
    $P_{k}^{f}\leftarrow \rho \circ \frac{\sum_{i}\left(x_{k}^{i,f}-m_{k}^{f}\right)\left(x_{k}^{i,f}-m_{k}^{f}\right)^\top}{N-1}$\\
    $K \leftarrow P^{f}_{k}H^{T} \left[HP^{f}_{k} H^{T}+R_{k}\right]^{-1}$\\
    \For{$i=1,\cdots,N$}{
        Sample $\eta^{i}_{k} \sim \mathcal{N}(0_q, \sigma^2I_q)$\\
        $y^{i}_{k}\leftarrow y_{k}+\eta^{i}_{k}$\\
        $x^{i,a}_{k} \leftarrow x^{i,f}_{k}+K\left[y^{i}_{k}-Hx^{i,f}_{k} \right]$
        }
    $\hat\pi_k \leftarrow\frac{1}{N}\sum_{i=1}^N \delta_{x^{i, a}_k}$
}
 \caption{EnKF with covariance localization}
\label{algo-enkf--probing-nfs} 
\end{algorithm}
We also use EnKF \cite{Evensen07} with observation perturbation, a Monte-Carlo method based on Kalman filter, where the mean and covariance are estimated from the ensemble. Covariance localization is implemented by constructing localization matrix ($\rho$ in algorithm~\ref{algo-enkf--probing-nfs}) using Gaspari-Cohn localization function with localization radius 2 to prevent filter divergence in high dimensions for small ensemble sizes \cite{farchi2018comparison}.
\subsection{Sinkhorn divergence}
\begin{algorithm}[!t]
Note the definition, $\text{LSE}_{k=1}^LV_k\stackrel{\text{def}}{=}\log\sum_{k=1}^L\exp(V_k)$.\\
Initialize $a_i\leftarrow0\;\forall\; i=1,\cdots,N$ and $ b_j\leftarrow 0,\;\forall\; j=1,\cdots,M$. Set $T$.\\
iteration $\leftarrow 0$\\
\While{$L_1$ relative error in $a > 0.1\%$ and iteration $< T$ }{
    \For{$i=1,\cdots, N$}{
        $a_i \leftarrow\
        -\varepsilon\text{LSE}_{k=1}^M\left(\log\nu_k+\frac{1}{\varepsilon}b_k - \frac{1}{\varepsilon}\|x_i-y_k\|_2^2 \right)$} %
    \For{$j=1,\cdots, M$}{
        $b_j \leftarrow\
        -\varepsilon\text{LSE}_{k=1}^N\left(\log\mu_k+\frac{1}{\varepsilon}a_k - \frac{1}{\varepsilon}\|x_k-y_j\|_2^2 \right)$}\
    iteration $\leftarrow$ iteration + 1}\
$\text{OT}_{\mu, \nu}\leftarrow\sum_{i=1}^N\mu_i a_i +\sum_{j=1}^M \nu_j b_j$\\
Initialize $a_i\leftarrow0\;\forall\; i=1,\cdots,N$ and $ b_j\leftarrow 0,\;\forall\; j=1,\cdots,M$.\\
\While{$L_1$ relative error in $a > 0.1\%$}{
    \For{$i=1,\cdots, N$}{
        $a_i \leftarrow
        \frac{1}{2}\left[a_i - \varepsilon\text{LSE}_{k=1}^N\left(\log\mu_k+\frac{1}{\varepsilon}a_k - \frac{1}{\varepsilon}\|x_i-x_k\|_2^2 \right)\right]$}
    } %
\While{$L_1$ relative error in $b > 0.1\%$}{
    \For{$j=1,\cdots, M$}{
        $b_j \leftarrow\
        \frac{1}{2}\left[b_j - \varepsilon\text{LSE}_{k=1}^M\left(\log\nu_k+\frac{1}{\varepsilon}b_k - \frac{1}{\varepsilon}\|y_j-y_k\|_2^2 \right)\right]$}
    } %
$S_\varepsilon\leftarrow \text{OT}_{\mu, \nu} - \sum_{i=1}^N\mu_i a_i -\sum_{j=1}^M \nu_j b_j$
 \caption{Computation of $S_\varepsilon$}
\label{algo-sink--probing-nfs}
\end{algorithm}
We choose the distance on $P(\mathbb R^d)$ in~\eqref{eq-stablaw--probing-nfs} to be the Wasserstein metric $W_2$. 
{
We approximate $W_2$ [see~\eqref{approx-w2--probing-nfs} later] by the Sinkhorn divergence $S_\varepsilon$ defined in~\eqref{def-sink--probing-nfs}, which in turn needs optimal transport distance $\text{OT}_\varepsilon$ defined in~\eqref{def-ot--probing-nfs}.}

Computation of optimal transport distances between probability measures has been a challenging task until recently. But entropy regularization has made solving the dual to the optimal transport problem tractable for sampling distributions through the use of Sinkhorn algorithm~\cite[and references therein]{genevay2019entropy}. To estimate $W_2$ we use Sinkhorn divergence $S_\varepsilon$ defined as follows \cite{feydy2019interpolating},
\begin{align}
    &\text{OT}_\varepsilon(\mu, \nu) \stackrel{\text{def}}{=} \min_{\pi \in \mathbb{S}}\left[\int\|x-y\|_2^2\,d\pi(x, y) + \varepsilon\text{KL}(\pi|\mu\otimes\nu)\right] \label{def-ot--probing-nfs}\\
    &\text{S}_\varepsilon \stackrel{\text{def}}{=} \text{OT}_\varepsilon(\mu, \nu) -\frac{1}{2}\text{OT}_\varepsilon(\mu, \mu)-\frac{1}{2}\text{OT}_\varepsilon(\nu, \nu) \label{def-sink--probing-nfs}
\end{align}
where the minimisation is over the set $\mathbb{S}$ of distributions $\pi$ with the first and second marginals being $\mu$ and $\nu$ respectively  
and $\text{KL}$ is the Kullbackâ€“Leibler divergence. We compute $S_\varepsilon(\mu, \nu)$ for probability measures of form $\mu=\sum_{i=1}^N\mu_i\delta_{x_i}$ and $\nu=\sum_{j=1}^M\nu_j\delta_{y_j}$ using algorithm~\ref{algo-sink--probing-nfs} from \cite{feydy2019interpolating}.
In our experiments $\mu_i=\frac{1}{N}$ and $\nu_j=\frac{1}{M}\; \forall\;i,j$. We use $\varepsilon=0.01$ and $T=200$ for all our experiments. { Note that \cite{genevay2019entropy, carlier2017convergence} 
\begin{equation}
    \lim_{\varepsilon\to0}\sqrt{S_\varepsilon(\mu, \nu)}= W_2(\mu, \nu) \,, \label{approx-w2--probing-nfs}
\end{equation}}
and $\varepsilon=0.01$ is a decent practical choice for estimating $W_2$. We use the notation $D_\varepsilon := \sqrt{S_\varepsilon}$ in the following sections. 
{
Although KL-divergence can be estimated with $k$-nearest neighbors based algorithms \cite{wang2009divergence}, this requires choosing the parameter $k$. $D_\varepsilon$ also requires choosing $\varepsilon$ but in this case convergence of $D_\varepsilon$ to $W_2$ as $\varepsilon\to 0$ and the appearance of division by $\varepsilon$ in the Sinkhorn algorithm are two opposing forces that help us come up with a reasonable choice. In absence of these constraints for KL-divergence, choosing $k$ becomes unmotivated, not to mention the estimated KL-divergence is more sensitive to the choice of $k$ than $D_\varepsilon$ is to the choice of $\varepsilon$. Combined with this practical nuance, the nice geometric  properties of $W_2$ e.g. metrizing the convergence in law \cite{feydy2019interpolating}, make it preferable to KL-divergence.}