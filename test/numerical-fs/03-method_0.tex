\subsection{Models} \label{ssec-models--numerical-fs}
We use two chaotic models in this chapter: (i) Lorenz-63~\cite[Chapter~14]{hirsch2012differential} with parameters $\rho = 28, \sigma = 10, \beta = \frac{8}{3}$ and (ii) 10 and 40-dimensional Lorenz-96~\cite{Lorenz96, kekem2018dynamics} with forcing constant $F=10$ and $F=8$ respectively. We observe the system every $0.1$ units of time which fixes the evolution function $f$. We observe alternate coordinates starting from the first coordinate, so
% which means
% $q = \left\lceil\frac{d}{2}\right\rceil$,
% % \begin{align}
% %     q = \left\lceil\frac{d}{2}\right\rceil
% % \end{align}
% so equation \eqref{eq-obs--numerical-fs} becomes
\begin{align}
    y_{k,j} = x_{k,2j-1} + \eta_{k,j}
\label{eq-altobs--numerical-fs} \end{align}
for $j=1, 2, \cdots, q = \left\lceil\frac{d}{2}\right\rceil$ and $\eta_{k, j}\sim\mathcal N(0, \sigma^2)$. Throughout the paper, we choose $\sigma^2=0.1$ or $\sigma^2 = 1.0$.
% for particle filter and $\sigma^2=1.0$ for ensemble Kalman fiter.
%-----------------------------------
\subsubsection{Data Generation}
After selecting a model we find a point on the corresponding attractor by randomly generating an initial point and evolving it according to $f$ for $10^5$ iterations. Starting from this point $x_0^{\text{true}}$ on the attractor, we generate a true trajectory according to \eqref{eq-state--numerical-fs} and then generate $10$ different observation realizations for the same trajectory according to \eqref{eq-altobs--numerical-fs}. 
%-----------------------------------
\subsubsection{Initial distributions}\label{ssec-init-dist--numerical-fs}
We use three initial conditions:
\begin{align}
    &\mu_1 = \mathcal{N}(x_0^{\text{true}}, 0.1\times I_d) \,, \nonumber \\
    &\mu_2 = \mathcal{N}(x_0^{\text{true}} + 2\times1_d, 0.5\times I_d) \,, \nonumber \\
    &\mu_3 = \mathcal{N}(x_0^{\text{true}} + 4\times1_d, I_d) \,,
\label{eq-3ic--numerical-fs} \end{align}
where $1_d$ is a $d$-dimensional vector with all entries $1$.
%% -- -- -- -- -- --
\subsection{Particle Filter} \label{ssec-pf--numerical-fs}
\begin{algorithm}[!t]
Initialize $N$ particles $\{x_0^i\}_{i=1}^N$ according to the initial distribution with equal weights $\left\{w_0^i=\frac{1}{N}\right\}_{i=1}^N$. Set $\tilde\sigma$. Below $S[i]$ denotes $i$-th element of $S$.\\
 \For{$k=0,\cdots,n$}{
      \If{$k>0$}{
      \For{$i=1,\cdots,N$}{
            $x^i_k\leftarrow f(S[i])$
            }
      }
      Sample $u\sim\mathcal{U}\left(0, \frac{1}{N}\right)$\\
      \For{$i=1,\cdots,N$}{
            $w^i_k\leftarrow p(y_k|x_k^i)$\\
            $U_{i} \leftarrow u + \frac{i-1}{N}$
            }
      $W\leftarrow\sum_{i=1}^Nw^i_k$\\
      \For{$i=1,\cdots,N$}{ 
            \If{$|\{U_j:\sum_{l=1}^{i-1}w^l_k \le WU_j\le\sum_{l=1}^{i}w^l_k\}|>0$}{
                    tag $x^i_k$ as significant
            }
            }
      Set $S\leftarrow\{x^{i_1}_k, x^{i_2}_k, \cdots, x^{i_m}_k\}$ as the set of  significant particles and compute $N_j\propto w^{i_j}_k:  \sum_{j=1}^mN_j = N$.\\
      \For{$j=1, \cdots, m$}{
            $S\leftarrow S\cup\{N_j-1\text{ samples from }\mathcal{N}(x^{i_j}_k, \tilde\sigma^2I_d)\}$
            }
    $\hat\pi_k \leftarrow\frac{1}{N}\sum_{i=1}^N\delta_{S[i]}$
  }
 \caption{BPF with offspring-based resampling}
\label{algo-bpf--numerical-fs}
\end{algorithm}
We use the bootstrap particle filter (BPF), algorithm~\ref{algo-bpf--numerical-fs}, resampling at every assimilation step. In order to avoid weight degeneracy in BPF for deterministic models, we incorporate post-regularization \cite{farchi2018comparison} in resampling steps by using an offspring based resampling strategy. We find the significant particles in the same manner as systematic resampling \cite{doucet2009tutorial} and then generate offspring for each of the significant particles by placing a Gaussian distribution with covariance $\tilde\sigma^2I_d$ around them. Number of offspring is set to be proportional to the weight of the particle.

In algorithm~\ref{algo-bpf--numerical-fs}, we use the convention that $\sum_{l=1}^0w^l_k=0$. It should be noted that larger values of $\tilde\sigma$ are needed to prevent filter collapse when working with fewer number of particles. In our experiments for Lorenz 63, $\tilde\sigma^2=0.1$ and for Lorenz 96, $\tilde\sigma^2=0.5$.
%% -- -- -- -- -- --
\subsection{Ensemble Kalman filter (EnKF)} \label{ssec-enkf--numerical-fs}
\begin{algorithm}[!t]
Initialize $N$ particles $\{x_0^i\}_{i=1}^N$ according to the initial distribution and set $x_0^{i,a}=x_0^i$ \\
\For{$k=1,\cdots,n$}{
%    \If{$k>0$}{
    \For{$i=1,\cdots,N$}{
        $x^{i,f}_{k}\leftarrow f(x_{k-1}^{i,a})$
%        }
    }
    $m_{k}^{f}\leftarrow \frac{1}{N}\sum_{i}x_{k}^{i,f}$\\ %\frac{\sum_{i}x_{k}^{i,f}}{N}$\\
    $P_{k}^{f}\leftarrow \rho \circ \frac{\sum_{i}\left(x_{k}^{i,f}-m_{k}^{f}\right)\left(x_{k}^{i,f}-m_{k}^{f}\right)^\top}{N-1}$\\
    % $P^{f}_{k} \leftarrow \rho \circ P^{f}_{k}$\\
    $K \leftarrow P^{f}_{k}H^{T} \left[HP^{f}_{k} H^{T}+R_{k}\right]^{-1}$\\
    \For{$i=1,\cdots,N$}{
        Sample $\eta^{i}_{k} \sim \mathcal{N}(0_q, \sigma^2I_q)$\\
        $y^{i}_{k}\leftarrow y_{k}+\eta^{i}_{k}$\\
    %     }
    % \For{$i=1,\cdots,N$}{
        $x^{i,a}_{k} \leftarrow x^{i,f}_{k}+K\left[y^{i}_{k}-Hx^{i,f}_{k} \right]$
        }
    $\hat\pi_k \leftarrow\frac{1}{N}\sum_{i=1}^N \delta_{x^{i, a}_k}$
}
% \For{$k=0,\cdots,n$}{
%     \If{$k>0$}{
%         \For{$i=1,\cdots,N$}{
%             $x^{i,f}_{k+1}\leftarrow f(x_{k}^{i,a})$
%         }
%     }
%     $m_{k+1}^{f}\leftarrow \frac{1}{N}\sum_{i}x_{k+1}^{i,f}$\\ %\frac{\sum_{i}x_{k+1}^{i,f}}{N}$\\
%     $P_{k+1}^{f}\leftarrow\frac{1}{N-1}\sum_{i}\left(x_{k+1}^{i,f}-m_{k+1}^{f}\right)\left(x_{k+1}^{i,f}-m_{k+1}^{f}\right)^\top$\\
%     Sample $\eta^{i}_{k} \sim \mathcal{N}(0_q, \sigma^2I_q)$\\
%     \For{$i=1,\cdots,N$}{
%         $y^{i}_{k+1}\leftarrow y_{k+1}+\eta^{i}_{k+1}$
%         }
%     $P^{f}_{k+1} \leftarrow \rho \circ P^{f}_{k+1}$
%     $K \leftarrow P^{f}_{k+1}H^{T} \left[HP^{f}_{k+1} H^{T}+R_{k}\right]^{-1}$\\
%     \For{$i=1,\cdots,N$}{
%         $x^{i,a}_{k+1} \leftarrow x^{i,f}_{k+1}+K\left[y^{i}_{k+1}-Hx^{i,f}_{k+1} \right]$
%         }
%     $\hat\pi_k \leftarrow\frac{1}{N}\sum_{i=1}^N \delta_{x^{i, a}_k}$
% }
 \caption{EnKF with covariance localization}
\label{algo-enkf--numerical-fs} 
\end{algorithm}
We also use EnKF \cite{Evensen07} with observation perturbation, a Monte-Carlo method based on Kalman filter, where the mean and covariance are estimated from the ensemble. Covariance localization is implemented by constructing localization matrix ($\rho$ in algorithm~\ref{algo-enkf--numerical-fs}) using Gaspari-Cohn localization function with localization radius 4 to prevent filter divergence in high dimensions for small ensemble sizes \cite{farchi2018comparison}.
%-----------------------------------
\subsection{Sinkhorn divergence}
\begin{algorithm}[!t]
Note the definition, $\text{LSE}_{k=1}^LV_k\stackrel{\text{def}}{=}\log\sum_{k=1}^L\exp(V_k)$.\\
Initialize $a_i\leftarrow0\;\forall\; i=1,\cdots,N$ and $ b_j\leftarrow 0,\;\forall\; j=1,\cdots,M$. Set $T$.\\
iteration $\leftarrow 0$\\
\While{$L_1$ relative error in $a > 0.1\%$ and iteration $< T$ }{
    \For{$i=1,\cdots, N$}{
        $a_i \leftarrow\
        -\varepsilon\text{LSE}_{k=1}^M\left(\log\nu_k+\frac{1}{\varepsilon}b_k - \frac{1}{\varepsilon}\|x_i-y_k\|_2^2 \right)$} %\frac{1}{\varepsilon}
    \For{$j=1,\cdots, M$}{
        $b_j \leftarrow\
        -\varepsilon\text{LSE}_{k=1}^N\left(\log\mu_k+\frac{1}{\varepsilon}a_k - \frac{1}{\varepsilon}\|x_k-y_j\|_2^2 \right)$}\
    iteration $\leftarrow$ iteration + 1}\
$\text{OT}_{\mu, \nu}\leftarrow\sum_{i=1}^N\mu_i a_i +\sum_{j=1}^M \nu_j b_j$\\
Initialize $a_i\leftarrow0\;\forall\; i=1,\cdots,N$ and $ b_j\leftarrow 0,\;\forall\; j=1,\cdots,M$.\\
\While{$L_1$ relative error in $a > 0.1\%$}{
    \For{$i=1,\cdots, N$}{
        $a_i \leftarrow
        \frac{1}{2}\left[a_i - \varepsilon\text{LSE}_{k=1}^N\left(\log\mu_k+\frac{1}{\varepsilon}a_k - \frac{1}{\varepsilon}\|x_i-x_k\|_2^2 \right)\right]$}
    } %\frac{1}{\varepsilon}
\While{$L_1$ relative error in $b > 0.1\%$}{
    \For{$j=1,\cdots, M$}{
        $b_j \leftarrow\
        \frac{1}{2}\left[b_j - \varepsilon\text{LSE}_{k=1}^M\left(\log\nu_k+\frac{1}{\varepsilon}b_k - \frac{1}{\varepsilon}\|y_j-y_k\|_2^2 \right)\right]$}
    } %\frac{1}{\varepsilon}
$S_\varepsilon\leftarrow \text{OT}_{\mu, \nu} - \sum_{i=1}^N\mu_i a_i -\sum_{j=1}^M \nu_j b_j$
 \caption{Computation of $S_\varepsilon$}
\label{algo-sink--numerical-fs}
\end{algorithm}
We choose the distance on $P(\mathbb R^d)$ in~\eqref{eq-stablaw--numerical-fs} to be the Wasserstein metric $W_2$. 
{
We approximate $W_2$ [see~\eqref{approx-w2--numerical-fs} later] by the Sinkhorn divergence $S_\varepsilon$ defined in~\eqref{def-sink--numerical-fs}, which in turn needs optimal transport distance $\text{OT}_\varepsilon$ defined in~\eqref{def-ot--numerical-fs}.}

Computation of optimal transport distances between probability measures has been a challenging task until recently. But entropy regularization has made solving the dual to the optimal transport problem tractable for sampling distributions through the use of Sinkhorn algorithm~\cite[and references therein]{genevay2019entropy}. To estimate $W_2$ we use Sinkhorn divergence $S_\varepsilon$ defined as follows \cite{feydy2019interpolating},
\begin{align}
    &\text{OT}_\varepsilon(\mu, \nu) \stackrel{\text{def}}{=} \min_{\pi \in \mathbb{S}}\left[\int\|x-y\|_2^2\,d\pi(x, y) + \varepsilon\text{KL}(\pi|\mu\otimes\nu)\right] \label{def-ot--numerical-fs}\\
    % &\text{OT}_\varepsilon(\mu, \nu) \stackrel{\text{def}}{=} \min_{\pi_1=\mu, \pi_2=\nu}\left[\int\|x-y\|_2^2\,d\pi(x, y) + \varepsilon\text{KL}(\pi|\mu\otimes\nu)\right]\\
    &\text{S}_\varepsilon \stackrel{\text{def}}{=} \text{OT}_\varepsilon(\mu, \nu) -\frac{1}{2}\text{OT}_\varepsilon(\mu, \mu)-\frac{1}{2}\text{OT}_\varepsilon(\nu, \nu) \label{def-sink--numerical-fs}
\end{align}
where the minimisation is over the set $\mathbb{S}$ of distributions $\pi$ with the first and second marginals being $\mu$ and $\nu$ respectively  
and $\text{KL}$ is the Kullback–Leibler divergence. We compute $S_\varepsilon(\mu, \nu)$ for probability measures of form $\mu=\sum_{i=1}^N\mu_i\delta_{x_i}$ and $\nu=\sum_{j=1}^M\nu_j\delta_{y_j}$ using algorithm~\ref{algo-sink--numerical-fs} from \cite{feydy2019interpolating}.
In our experiments $\mu_i=\frac{1}{N}$ and $\nu_j=\frac{1}{M}\; \forall\;i,j$. We use $\varepsilon=0.01$ and $T=200$ for all our experiments. { Note that \cite{genevay2019entropy, carlier2017convergence} 
\begin{equation}
    \lim_{\varepsilon\to0}\sqrt{S_\varepsilon(\mu, \nu)}= W_2(\mu, \nu) \,, \label{approx-w2--numerical-fs}
\end{equation}}
and $\varepsilon=0.01$ is a decent practical choice for estimating $W_2$. We use the notation $D_\varepsilon := \sqrt{S_\varepsilon}$ in the following sections. 
{
Although KL-divergence can be estimated with $k$-nearest neighbors based algorithms \cite{wang2009divergence}, this requires choosing the parameter $k$. $D_\varepsilon$ also requires choosing $\varepsilon$ but in this case convergence of $D_\varepsilon$ to $W_2$ as $\varepsilon\to 0$ and the appearance of division by $\varepsilon$ in the Sinkhorn algorithm are two opposing forces that help us come up with a reasonable choice. In absence of these constraints for KL-divergence, choosing $k$ becomes unmotivated, not to mention the estimated KL-divergence is more sensitive to the choice of $k$ than $D_\varepsilon$ is to the choice of $\varepsilon$. Combined with this practical nuance, the nice geometric  properties of $W_2$ e.g. metrizing the convergence in law \cite{feydy2019interpolating}, make it preferable to KL-divergence.}