Many real world problems can be modelled as the response of nonlinear systems to random excitations and such systems have been a topic of interest for a long time. Stochastic differential equations (SDE) provide the natural language for describing these systems. Although SDEs have their origins in the study of Brownian motion by Einstein and Smoluchowski, it was Itô who first developed the mathematical theory. Since then SDEs have extensively appeared in physics~\cite{lelievre2016partial, strauss2017hitch, ivanov1980method}, biology~\cite{allen2010introduction}, mathematical finance~\cite{delong2013backward, karoui1997non}, and many other fields~\cite{oksendal2003stochastic, gardiner2009stochastic}. 
The probability density associated with an Itô SDE evolves in time according to a Fokker-Planck equation (FPE) or Kolmogorov forward equation. A stationary FPE (SFPE) can be solved analytically when the corresponding Itô SDE has a drift term that can be represented as the gradient of some potential~\cite{risken1996fokker}. 
But the same is not true when the drift is not of the aforementioned form. In either of these two cases, the time-dependent FPE does not admit a closed form solution in general even when the drift is integrable. Since the solution of an FPE is a probability density, the boundary condition is replaced by an integral condition for normalization, which is extremely hard to implement in dimensions larger than two.

In recent times deep learning has been successfully used to solve high-dimensional PDEs~\cite{sirignano2018dgm, han2018solving, raissi2019physics}. Although universal approximation theorems~\cite{pinkus1999approximation, lu2020universal, de2021approximation, kovachki2021universal, de2022error, mishra2022estimates} guarantee existence of neural networks that approximate the true solution well, due to the non-convex nature of loss functions one can not guarantee convergence of neural networks to the true solution during training in many instances~\cite{krishnapriyan2021characterizing, basir2022investigating}.
Moreover, these methods are almost always used for PDEs with simple boundary conditions not containing integral terms which makes applying them for FPEs challenging. But even though deep learning solutions to PDEs is fraught with challenges, it is a worthwhile paradigm to work in while dealing with high-dimensional PDEs for the following reasons. 
Most deep learning methods are mesh-free~\cite{blechschmidt2021three} and have the potential to deal with the curse of dimensionality much better than classical methods \cite{cioica2022deep}. Moreover, some of them focus on computing pointwise solutions to PDEs~\cite{han2018solving} which albeit non-standard, might be the only practical and efficient approach in high dimensions. 

The goal of this chapter is to devise a reliable, mesh-free deep learning algorithm to find non-trivial zeros of high-dimensional Fokker-Planck operators. In a sequel we will devise a method for solving high-dimensional time-dependent FPEs using these zeros. We focus on examples we problems where the underlying ODE system possesses a global attractor. \amit{.....Although our algorithm is capable of handling any non-solenoidal drift function -- is it really??? some implicit assumption about compact support is needed, if not explicit attractor for the ODE......} These systems are often used to make simple models in the earth sciences and provide ideal test cases for non-linear filtering algorithms~\cite{carrassi2022data}. We solve 2, 3, 4, 6, 8, and 10 dimensional problems with our method. We compare our method with Monte Carlo for $d=2$, investigate how the loss function and the distance from the true solution are related to each other, and explore how our method scales with dimension.

