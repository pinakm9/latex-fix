In this section we outline the algorithm for learning zeros of FPOs. But before that we go through the primary challenges and ways to mitigate them.

\subsection{Unboundedness of the problem domain}\label{ssec-unbounded-domain--steady-fp} We can try the same procedure as outlined in section~\ref{sec-learning--steady-fp} to find a non-trivial zero of $\mathcal L$. But computationally we can only deal with a bounded domain. Hence we focus on a compact domain which contains most of the mass of the solution to \eqref{eq:SFPE-0--steady-fp}. We refer to this domain as the \textit{domain of interest} $\Omega_I$ in the following discussion. \textcolor{magenta}{We note that the support of non-trivial zeros of $\mathcal L$ will usually be unbounded and of course we do not know the domain that may contain most of the mass. Thus the choice of $\Omega_I$ needs to be informed by some a priori knowledge of the solution, which in the examples we discuss is related to some attracting set of the deterministic system associated to the drift term $\mu$, i.e., the first term in~\eqref{eq-sde--steady-fp}. We do not need a precise knowledge of such an attracting set. But the smaller the domain $\Omega_I$, the more efficient will be the proposed method which requires uniform samples from $\Omega_I$.}
%{We note two points about the choice of $\Omega_I$: (i) The support of non-trivial zeros of $\mathcal L$ will usually be unbounded}


\subsection{Existence of the trivial solution}\label{ssec-exist-0--steady-fp} \textcolor{magenta}{Since $\mathcal L$ is a linear operator, zero is a trivial solution: $\mathcal L 0 = 0$. We also note that if $\nabla \cdot \mu \not\equiv 0$, then no other constant function (other than zero) is a zero of $\mathcal L$.} Since we want to find a non-trivial zero of $\mathcal L$, we would like avoid the learning the zero function during the training of the network. To deal with this problem \cite{zhai2022deep} added a regularization term that used approximate solutions of \eqref{eq:SFPE-0--steady-fp} found using Monte-Carlo. Here we propose a method that does not require a priori knowing an approximate solution. Consider the operator $\mathcal L_{\rm log}$ instead.
\begin{align}
    \mathcal L_{\rm log}f \stackrel{\rm def}{=} e^{-f}\mathcal L e^f\label{eq:def-log-FPO--steady-fp}
\end{align}
\textcolor{magenta}{Note that if $f$ is a zero of $\mathcal L_{\rm log}$, then $p = e^f$ is a zero of $\mathcal{L}$. Further, if $f$ is bounded below on some domain within $\Omega_I$, then $p$ is a non-trivial zero of $\mathcal{L}$.} Thus we can look for a zero of $\mathcal L_{\rm log}$ to find a non-trivial zero of $\mathcal L$. Recalling \eqref{eq:log-factor-V--steady-fp} we see that,
\begin{align}
    \mathcal L_{\rm log}f=
    -\nabla\cdot \mu - \mu \cdot \nabla f + D\left(\|\nabla f\|_2^2 + \Delta f\right)\label{eq:log-FPO--steady-fp}
\end{align}
We again note that when $\nabla\cdot\mu\not\equiv0$, then any constant function can not be a zero of $\mathcal L_{\rm log}$. 


OLDER VERSION (the iff below is not correct; also any (nonzero) constant is NOT a zero of L either so the first argument is not very strong  ...) ---- Recalling \eqref{eq:log-factor-V--steady-fp} we see that,
\begin{align}
    \mathcal L_{\rm log}f=
    -\nabla\cdot \mu - \mu \cdot \nabla f + D\left(\|\nabla f\|_2^2 + \Delta f\right)\label{eq:log-FPO-old--steady-fp}
\end{align}
Since $\nabla\cdot\mu\not\equiv0$, any constant function can not be a zero of $\mathcal L_{\rm log}$. $p$ is a zero of $\mathcal L$ iff either $p\equiv0$ or $\log p$ is a zero of $\mathcal L_{\rm log}$. So we can look for a zero of $\mathcal L_{\rm log}$ to find a non-trivial zero of $\mathcal L$. ---- END OLDER VERSION

\subsection{The steady state algorithm}
The procedure outlined in section~\ref{sec-learning--steady-fp} together with the modifications in sections~\ref{ssec-unbounded-domain--steady-fp}-\ref{ssec-exist-0--steady-fp} immediately yield the following loss function. \textcolor{magenta}{ADD the argument of LHS below (done)}
\begin{align}
    L_{\log}(\theta) = \frac{1}{N}\sum_{i=1}^N\mathcal L_{\rm log}(n_\theta(\mathbf x_i))^2\label{eq:def-steady-loss--steady-fp}
\end{align}
where $\{\mathbf x_i\}_{i=1}^N$ is a uniform sample from $\Omega_I$. Accordingly, the final procedure for finding a non-trivial zero of $\mathcal L$ is given in algorithm~\ref{algo:steady--steady-fp}.
%%%
\begin{algorithm}[t!]
Select the desired architecture for $n_\theta$.\\
Select resampling interval $\tau$.\\
Select an adaptive learning rate $\delta(k)$ and the number of training iterations $E$. 
Sample $\{\mathbf x_i\}_{i=1}^N$ from $\Omega_I$, the domain of interest.\\
\For {$k=1,2\cdots, E$}{
Compute $\nabla_\theta L_{\log}=\frac{1}{N}\sum_{i=1}^N\nabla_\theta(\mathcal{L}_{\log}(n_\theta(\mathbf x_i))^2)$\\
where $\mathcal L_{\log}f = -\nabla\cdot \mu - \mu \cdot \nabla f + D\left(\|\nabla f\|_2^2 + \Delta f\right)$\\
Update $\theta\leftarrow\theta - \delta(k) \nabla_{\theta}L_{\log}$\\
\If{$k\text{ is divisible by }\tau$}{Resample $\{\mathbf x_i\}_{i=1}^N$ from $\Omega_I$}
}
$e^{n_\theta(\mathbf x)}$ is a non-trivial zero of $\mathcal{L}$.\\
Optional: Approximate $Z\leftarrow\int_{\mathbb R^d}e^{n_\theta(\mathbf x)}\,d\mathbf x$.\\
$\frac{1}{Z}e^{n_\theta(\mathbf x)}$ is the learned, normalized steady state.
\caption{The steady state algorithm}
\label{algo:steady--steady-fp}
\end{algorithm}
%%%
In the following sections we describe in detail the network architecture and optimizer used in our experiments. Since the final solution is represented as $e^{n_\theta(\mathbf x)}$, we have automatically secured positivity of the solution.

\textcolor{magenta}{in general, is it not better to add another stopping criterion - how small is $L_{log}(\theta)$??? should we mention that somewhere???}

\subsection{Architecture}\label{ssec-architecture--steady-fp}
We choose the widely used LSTM \cite{sherstinsky2020fundamentals}, \cite{vennerod2021long} architecture described below for our experiments. This type of architecture rose to prominence in deep learning because of their ability to deal with the vanishing gradient problem, see section IV of \cite{sherstinsky2020fundamentals}, section 2.2 of \cite{vennerod2021long}. A variant of this architecture has also been used to solve PDEs \cite{sirignano2018dgm}. This kind of architectures have been shown to be universal approximators \cite{schafer2006recurrent}. We choose this architecture simply because of how \textit{expressive} they are. By  expressivity of an architecture we imply its ability to approximate a wide range of functions and experts have attempted to formalize this notion in different ways in recent times \cite{lu2017expressive}, \cite{raghu2016survey},  \cite{raghu2017expressive}. There are architectures that are probability densities by design i.e. the normalization constraint in \eqref{eq:SFPE-0--steady-fp} is automatically satisfied for them, see for example \cite{uria2013rnade}, \cite{papamakarios2019neural}. But our experiments suggest these architectures are not expressive enough to learn solutions to PDEs efficiently since the normalization constraint makes their structure too rigid. Other than the difficulty in implementing the normalization constraint numerically, this is another reason why we choose to focus on learning a non-trivial zero of $\mathcal L$ rather than solving \eqref{eq:SFPE-0--steady-fp}. LSTM networks on the other hand are expressive enough to solve all the problems listed in section~\ref{sec-examples--steady-fp}.

Below we define the our architecture in detail. Here $\mathbf 0_k$ implies a zero vector of dimension $k$ and $\odot$ implies the Hadamard product.
\begin{align}
    i\in\{1,2,&\cdots, L\}\\
    \mathtt c_0(\mathbf x)\stackrel{\rm def}{=}&\,\mathbf 0_m\label{eq:layer-c0--steady-fp}\\
    \mathtt h_0(\mathbf x)\stackrel{\rm def}{=}&\,\mathbf 0_d\label{eq:layer-h0--steady-fp}\\
    \mathtt f_i(\mathbf x) \stackrel{\rm def}{=}& \mathtt A(\mathtt W_f^{(i)}\mathbf x + \mathtt U_f^{(i)}h_{i-1}(\mathbf x) + \mathtt b_f^{(i)})\label{eq:layer-f--steady-fp}\\
    \mathtt g_i(\mathbf x) \stackrel{\rm def}{=}& \mathtt A(\mathtt W_g^{(i)}\mathbf x + \mathtt U_g^{(i)}h_{i-1}(\mathbf x) + \mathtt b_g^{(i)})\label{eq:layer-g--steady-fp}\\
    \mathtt r_i(\mathbf x) \stackrel{\rm def}{=}& \mathtt A(\mathtt W_r^{(i)}\mathbf x + \mathtt U_r^{(i)}h_{i-1}(\mathbf x) + \mathtt b_r^{(i)})\label{eq:layer-r--steady-fp}\\
    \mathtt s_i(\mathbf x) \stackrel{\rm def}{=}& \mathtt A(\mathtt W_s^{(i)}\mathbf x + \mathtt U_s^{(i)}h_{i-1}(\mathbf x) + \mathtt b_s^{(i)})\label{eq:layer-s--steady-fp}\\
    \mathtt c_i(\mathbf x) \stackrel{\rm def}{=}&  \mathtt f_i(\mathbf x)\odot \mathtt c_{i-1}(\mathbf x) + \mathtt g_i(\mathbf x)\odot s_i(\mathbf x)\label{eq:layer-c--steady-fp}\\
    \mathtt h_i(\mathbf x) \stackrel{\rm def}{=}& \mathtt r_i(\mathbf x)\odot \mathtt A(\mathtt c_i(\mathbf x))\label{eq:layer-h--steady-fp}\\
    \mathtt d_L(\mathbf x)\stackrel{\rm def}{=}&\mathtt W^\top\mathbf x + \mathtt b\label{eq:layer-final--steady-fp}\\
    n^{\rm LSTM}_\theta \stackrel{\rm def}{=}& \mathtt d_L\circ \mathtt h_L\label{eq:def-LSTM--steady-fp} 
\end{align}
Here $\{\mathtt f_i, \mathtt g_i, \mathtt r_i, \mathtt s_i, \mathtt c_i, \mathtt h_i: i=1,\cdots,L\}\cup\{\mathtt d_L\}$ are the hidden layers and
\begin{align}
    \theta=\{\mathtt W_f^{(i)}, \mathtt U_f^{(i)}, \mathtt b_f^{(i)}, \mathtt W_g^{(i)}, \mathtt U_g^{(i)}, \mathtt b_g^{(i)}, \mathtt W_r^{(i)}, \mathtt U_r^{(i)}, \mathtt b_r^{(i)}, \mathtt W_s^{(i)}, \mathtt U_s^{(i)}, \mathtt b_s^{(i)}:
i=1,\cdots,L\}\cup\{\mathtt W, \mathtt b\}\label{eq:theta-composition--steady-fp}
\end{align}is the set of the trainable parameters. The dimensions of these parameters are given below,
\begin{align}
   \mathtt W_f^{(i)}, 
   \mathtt W_g^{(i)},  \mathtt W_r^{(i)},  \mathtt W_s^{(i)} \in&\;
   \mathbb R^{m\times d}\\
   \mathtt U_f^{(i)},
\mathtt U_g^{(i)},
\mathtt U_r^{(i)},
\mathtt U_s^{(i)}\in&
   \begin{cases}\mathbb R^{m\times d},\quad\text{ if }i=1 \\
   \mathbb R^{m\times m},\quad\text{otherwise}
   \end{cases}\\
   \mathtt b_f^{(i)},\mathtt b_g^{(i)},\mathtt b_r^{(i)},\mathtt b_s^{(i)}\in&\;\mathbb R^m\\
   \mathtt W\in\mathbb R^{m}, \mathtt b\in&\;\mathbb R
\end{align}
which implies the size of the network or cardinality of $\theta$ is
\begin{align}
    C=4m[d(L+1)+m(L-1)]+5m+1\label{eq:network-size--steady-fp}
\end{align}
Note that \eqref{eq:network-size--steady-fp} implies the size of the network grows only linearly with dimension which is an important factor for mitigating the curse of dimensionality. We use elementwise $\tanh$ as our activation function,
\begin{align}
    \mathtt A=\tanh\label{eq:activation-choice--steady-fp}
\end{align}
We use $m=50$ and $L=3$ for our experiments which implies our network has $6L+1=19$ hidden layers. We use the very popular Xavier or Glorot initialization \cite{glorot2010understanding}, \cite{datta2020survey} to initialize $\theta$. With that, the description of our architecture is complete. 

\subsection{Optimization}\label{ssec-optimization--steady-fp}
In our experiments we use the ubiquitous Adam optimizer \cite{kingma2014adam} which is often used in the PDE solving literature \cite{han2018solving}, \cite{zhai2022deep}, \cite{sirignano2018dgm}. We use a piece-wise linear decaying learning rate. Below $k$ denotes the training iteration and $\delta(k)$ is the learning rate.
\begin{align}
    \delta(k)=\begin{cases}
        5\times10^{-3},\quad\text{if } k<1000
        \\
        1\times10^{-3},\quad\text{if }1000\le k<2000\\
        5\times10^{-4},\quad\text{if }2000\le k<10000\\
        1\times10^{-4},\quad\text{if }k\ge10000\\
        \end{cases}\label{eq:learning-rate--steady-fp}
\end{align}
We stop training after reaching a certain number of iterations $E$ which varies depending on the problem. In all our experiments we use $N=1000$ as the sample size and $\tau=10$ as the resampling interval for algorithm~\ref{algo:steady--steady-fp}.

