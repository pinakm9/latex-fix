In this section we describe the general process of \textit{learning} a solution to a partial differential equation. The strategy described here will be an integral part of the final algorithm. In what follows next, we see how to solve a generic PDE independent of time on a bounded domain with a Dirichlet boundary condition in a \textit{physics-informed} manner.  The interested reader can see \cite{raissi2019physics}, \cite{blechschmidt2021three}, \cite{sirignano2018dgm} for more discussions. In the next few subsections we keep simplifying our PDE problem until it finally becomes solvable on a computer.

\subsection{From PDE to optimization problem} \textcolor{red}{why introduce boundary conditions here?; a comment about normalization is needed} In the context of machine learning, \textit{learning} refers to solving an optimization problem. So to solve our PDE with deep learning we first transform it into an optimization problem. For this purpose, we recall the 2nd order PDE we want to solve can be written as 
\begin{equation}
\begin{aligned}
    &\mathcal Lf(\mathbf x) = 0,\quad \mathbf x\in\Omega \,, \qquad \textrm{and} \qquad
    &f(\mathbf x) = g(\mathbf x),\quad \mathbf x\in\partial\Omega \,,
\end{aligned}\label{eq:generic-pde--steady-fp}
\end{equation}
and we are interested in finding a solution in $W^{1,2}_{\rm loc}(\Omega)\cap C^2(\Omega)$. Instead of trying to solve \eqref{eq:generic-pde--steady-fp} a popular strategy is to try to solve the following problem (see for example \cite{sirignano2018dgm}),
\begin{align}
    f^* = \underset{f\in W^{1,2}_{\rm loc}(\Omega)\cap C^2(\Omega)}{\rm arg\,inf}\left[\int_\Omega (\mathcal L f)^2 + \int_{\partial\Omega}(f-g)^2\right] \,.\label{eq:generic-pde-opt--steady-fp}
\end{align}
The choice of function space ensures one-to-one correspondence between the solutions of the PDE and the optimization problem.

\subsection{From infinite-dimensional search space to finite-dimensional search space}\label{ssec-infinite-to-finite--steady-fp} To solve a problem on a machine with finite resources we need to find a finite dimensional (and in fact, a finite) approximation of the infinite dimensional aspects of the problem. We then solve the approximate, finite problem and preferably also estimate how well the finite solution approximates the solution to the original problem.

In particular, we will replace our search space $W^{1,2}_{\rm loc}(\Omega)\cap  C^2(\Omega)$ with a finite dimensional one by appealing to universal approximation theorems that say that neural networks of even the simplest architectures are dense in continuous functions, see for example theorem 3.2 in \cite{kidger2020universal} or proposition 3.7 in \cite{pinkus1999approximation}. Universal approximation theorems typically allow networks to have either arbitrary depth or arbitrary width in order to achieve density \cite{pinkus1999approximation}, \cite{de2021approximation}. But the sets of neural networks with arbitrary depth or width are still infinite dimensional and therefore are infeasible to work with. In practice, we fix an architecture $\mathcal A$ with a fixed number of layers and trainable parameters and work with the following set instead.
\begin{align}
    S_{\mathcal A}\stackrel{\rm def}{=}\{n^{\mathcal A}_\theta: \theta\in \mathbb R^C\}\label{eq:search-space-net--steady-fp}
\end{align}
Here $n^{\mathcal A}_\theta$ is a network with architecture $\mathcal A$ with trainable parameters $\theta$ and $C$ is the total number of trainable parameters or the size of $\theta$. Since $C$ is fixed, $S_{\mathcal A}$ \textcolor{magenta}{is a subset of} \textcolor{lightgray}{has a one-to-one correspondence with} $\mathbb R^C$ and therefore is finite-dimensional. Even though we lose the density argument while working with $\theta$ of fixed size, in recent times it has been shown that sets like $S_\mathcal A$ are not closed in $W^{1, 2}(\Omega)$ and $n^{\mathcal A}_\theta$ can be used as a good function approximator, see \cite{mahan2021nonclosedness} for a detailed discussion. \textcolor{magenta}{need to give specific section or theorem etc. in mahan2021nonclosedness} In the following discussion we suppress the architecture and use $n^{\mathcal A}_\theta$ and $n_\theta$ interchangably for notational convenience. After restricting our search space to \eqref{eq:search-space-net--steady-fp},  our optimization problem becomes,
\begin{align}
    \theta^* = \underset{\theta\in\mathbb R^C}{\rm arg\,inf}\left[\int_{\Omega} (\mathcal L n_\theta)^2 + \int_{\partial\Omega}(n_\theta-g)^2\right] \,.\label{eq:generic-pde-opt-theta--steady-fp}
\end{align}
and the corresponding $n_{\theta^*}$ approximates the solution $f^*$ to the problem~\eqref{eq:generic-pde-opt--steady-fp}.

\subsection{From integrals to sums} 
When the domain $\Omega$ is high dimensional, computation of the integrals in \eqref{eq:generic-pde-opt-theta--steady-fp} will be extremely challenging. To deal with this we will replace the integrals in \eqref{eq:generic-pde-opt-theta--steady-fp} with Monte-Carlo sums as follows,
\begin{align}
    \theta^* = \underset{\theta\in\mathbb R^C}{\rm arg\,inf}\left[\frac{1}{N}\sum_{j=1}^N (\mathcal L n_\theta(\mathbf x_j))^2 + \frac{1}{M}\sum_{j=1}^M(n_\theta(\mathbf y_j)-g(\mathbf y_j))^2\right]\label{eq:generic-opt-final--steady-fp}
\end{align}
where $\{\mathbf x_j\}_{j=1}^N$, $\{\mathbf y_j\}_{j=1}^M$ are uniform samples from $\Omega$ and $\partial \Omega$ respectively. In this case, \eqref{eq:generic-opt-final--steady-fp} can interpreted as trying to find a network that satisfies the original problem~\eqref{eq:generic-pde--steady-fp} at the specified points $\{\mathbf x_j\}_{j=1}^N, \{\mathbf y_j\}_{j=1}^M$, which we can refer to as \textit{collocation points}. 

\subsection{Finding the optimal parameters}\label{ssec-finding-theta--steady-fp}
Having transformed the problem~\eqref{eq:generic-pde-opt--steady-fp} to the one stated above, we perform gradient descent with respect to $\theta$ to find the optimal network for the problem \eqref{eq:generic-opt-final--steady-fp}. The Monte-Carlo sample sizes is dictated by the hardware available.  In our experiments $N = M = 1000$. In many cases, these choices may not be enough to approximate the original integrals sufficiently well, as is the case in the examples in this chapter and in general in most problems of interest. In order to overcome this limitation and in order to the learn the solution on the entire domain as thoroughly as possible, we resample the domain every few training iterations. Thus, even though we are limited in sample size by our hardware, we can shift the burden on space or memory to time or number of training iterations, in order to adequately sample the entire domain. This principle of space-time trade-off is ubiquitous in machine learning \cite{buduma2022fundamentals} and comes in many different flavours like mini-batch gradient descent, stochastic gradient descent etc. Even though in this paradigm we are not training our network with typical input-output pairs, our method can be thought of as a variant of the mini-batch gradient descent.

\subsection{Rationale for deep learning}
In this context of our problem, deep learning refers to learning an approximate solution to \eqref{eq:generic-pde--steady-fp} with the outlined method with an architecture $\mathcal A$ that is \textit{deep} or has many hidden layers. Deep networks are more efficient as approximators than shallow networks in the sense that they require far fewer number of trainable parameters to achieve the same level of approximation. For a discussion see section 5 of \cite{holstermann2023expressive} or section 4 of \cite{lu2017expressive}. Now that we have described the general procedure of \textit{deep learning} a solution to a PDE, we will pause briefly to point out some benefits and demerits of this approach. Deep learning has, like any other method some disadvantages. 
\begin{itemize}
    \item Deep learning is slower and less accurate for lower dimensional problems for which standard solvers exist and have been in consistent development for many decades. 
    \item Most modern GPUs are optimized for computation with single precision or float32 numbers. This is efficient for rendering polygons or other image processing tasks which are the primary reasons GPUs were invented~\cite{peddie2023history} but float32 might not be accurate enough for scientific computing. Although not ideal, this problem will most likely disappear in the future. \textcolor{magenta}{why will it disappear? people will stop requiring high precision or ``hardware will get better?''}
    \item The objective or \textit{loss} function used in a typical problem might not be convex and hence difficult to deal with~\cite{krishnapriyan2021characterizing, basir2022investigating}, due to multiple local minima. 
\end{itemize}
But even with these disadvantages, the benefits of deep learning make it a worthwhile tool for solving PDEs.
\begin{itemize}
    \item Since we don't need to deal with meshes or grids in this method, we can mitigate the curse of dimensionality in memory. It will be clear from our experiments that the size of the network $C$ does not need to grow exponentially with the dimensions. This method lets one compute the solution at collocation points but if one wants to compute the solution over the entire domain, one needs to sample the entire domain thoroughly which can be done in a sequential manner without requiring more memory as discussed in~\ref{ssec-finding-theta--steady-fp}.
    \item All derivatives are computed with automatic differentiation and therefore are accurate up to floating point errors. Moreover, finite difference schemes do not satisfy some fundamental properties of differentiation e.g. the product rule \cite{ranocha2019mimetic}. With automatic differentiation one does not have to deal with such problems. 
    \item If one computes the solution over the entire domain, the solution is obtained in a functional form which can be differentiated, integrated etc.
    \item Other than a method for sampling no modifications are required for accommodating different domains. 
\end{itemize}
